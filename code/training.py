# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q8ieC63Q8GcNLaSSzVC4bwJFFo5Qj9KX
"""

import time

import matplotlib.pyplot as plt  # for plotting
import torch
import torch.nn as nn
import torch.optim as optim  # for gradient descent

import CNN_Model
import data_loading

torch.manual_seed(1)  # set the random seed
use_cuda = False


def get_model_name(name, batch_size, learning_rate, epoch):
    """ Generate a name for the model consisting of all the hyperparameter values"""

    path = "model_{0}_bs{1}_lr{2}_epoch{3}".format(name,
                                                   batch_size,
                                                   learning_rate,
                                                   epoch)
    return path


def get_accuracy(model, dataloader):
    correct = 0
    total = 0
    for imgs, labels in dataloader:
        #############################################
        # To Enable GPU Usage
        if use_cuda and torch.cuda.is_available():
            torch.cuda.empty_cache()
            imgs = imgs.cuda()
            labels = labels.cuda()
        #############################################
        output = model(imgs)

        # select index with maximum prediction score
        pred = output.max(1, keepdim=True)[1]
        correct += pred.eq(labels.view_as(pred)).sum().item()
        total += imgs.shape[0]
    return correct / total


def train(model, trainData, valData, batch_size=10, learning_rate=0.01, num_epochs=30):
    train_loader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True)
    val_loader = torch.utils.data.DataLoader(valData, batch_size=batch_size, shuffle=True)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)

    torch.manual_seed(1000)

    iters, losses, train_acc, val_acc = [], [], [], []
    epochs = range(num_epochs)

    # training
    n = 0  # the number of iterations
    start_time = time.time()

    for epoch in range(num_epochs):
        for imgs, labels in iter(train_loader):

            #############################################
            # To Enable GPU Usage
            if use_cuda and torch.cuda.is_available():
                torch.cuda.empty_cache()
                imgs = imgs.cuda()
                labels = labels.cuda()
            #############################################

            out = model(imgs)  # forward pass

            loss = criterion(out, labels)  # compute the total loss
            loss.backward()  # backward pass (compute parameter updates)
            optimizer.step()  # make the updates for each parameter
            optimizer.zero_grad()  # a clean up step for PyTorch

            # save the current training information
            iters.append(n)
            losses.append(float(loss) / batch_size)  # compute *average* loss
            n += 1

        train_acc.append(get_accuracy(model, train_loader))  # compute training accuracy
        val_acc.append(get_accuracy(model, val_loader))  # compute validation accuracy

        print("Epoch {}: Train acc: {} Validation acc: {}".format(
            epoch,
            train_acc[-1],
            val_acc[-1]))

        model_path = "./models/state_dict/" + str(get_model_name(model.name, batch_size, learning_rate, epoch))
        torch.save(model.state_dict(), model_path)

    print('Finished Training')
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("Total time elapsed: {:.2f} seconds".format(elapsed_time))

    # plotting training curve of Loss and iterations
    plt.title("Training Curve")
    plt.plot(iters, losses, label="Train")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.savefig("./models/plots/model_{}_{}_{}_Train_Loss.png".format(model.name, batch_size, learning_rate))
    plt.show()

    # plotting training curve of training accuracy and iterations
    plt.title("Training Curve")
    plt.plot(epochs, train_acc, label="Train")
    plt.plot(epochs, val_acc, label="Validation")
    plt.xlabel("Epochs")
    plt.ylabel("Training Accuracy")
    plt.legend(loc='best')
    plt.savefig("./models/plots/model_{}_{}_{}_Train_Val_Accuracy.png".format(model.name, batch_size, learning_rate))
    plt.show()

    print("Final Training Accuracy: {}".format(train_acc[-1]))
    print("Final Validation Accuracy: {}".format(val_acc[-1]))


def start_training(batch_size, lr, epoch):
    train_data, val_data = data_loading.load_train_val_data()
    CNN = CNN_Model.CNN_Spoken_Digit()
    if use_cuda and torch.cuda.is_available():
        torch.cuda.empty_cache()
        CNN.cuda()
        print("Training on GPU...")
    train(CNN, train_data, val_data, batch_size, lr, epoch)


def show_model_test_accuracy(bs, lr, epoch):
    model = CNN_Model.CNN_Spoken_Digit()
    modelPath = "./models/state_dict/" + str(get_model_name("CNN_Spoken_Digit", bs, lr, epoch))
    state = torch.load(modelPath)
    model.load_state_dict(state)
    testLoader = data_loading.load_test_data_loader()
    acc = get_accuracy(model, testLoader)
    print("{} with bs={} lr={} epoch={} test accuracy: {}".format(model.name, bs, lr, epoch, acc))


#################
train_mode = True
#################

batch_size = 128
lr = 0.0005
epoch = 10

if train_mode:
    start_training(batch_size, lr, epoch)

show_model_test_accuracy(batch_size, lr, epoch - 1)  # default load to last epoch
